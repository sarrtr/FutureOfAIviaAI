{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install torch-geometric\n",
    "!pip install networkx scikit-learn numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_predictions_path = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_features(G: nx.Graph, u: int, v: int, max_shortest=6):\n",
    "    \"\"\"\n",
    "    Computes set of structural graph features for a node pair (u, v):\n",
    "        1. Common Neighbors: Number of nodes connected to both u and v\n",
    "        2. Jaccard Coefficient: Normalized measure of neighborhood overlap\n",
    "        3. Adamic-Adar Index: Weighted version of common neighbors (favors rare neighbors)\n",
    "        4. Node Degrees: Connectivity information for both nodes\n",
    "        5. Shortest Path Length: Distance between nodes (capped for efficiency)\n",
    "    \"\"\"\n",
    "    # Common Neighbors\n",
    "    try:\n",
    "        cn = len(list(nx.common_neighbors(G, u, v)))\n",
    "    except Exception:\n",
    "        cn = 0\n",
    "    \n",
    "    # Jaccard Coefficient\n",
    "    j = 0.0\n",
    "    try:\n",
    "        jc = next(nx.jaccard_coefficient(G, [(u,v)]))[2]\n",
    "        j = jc if jc is not None else 0.0\n",
    "    except Exception:\n",
    "        j = 0.0\n",
    "    \n",
    "    # Adamic-Adar Index\n",
    "    aa = 0.0\n",
    "    try:\n",
    "        aa = next(nx.adamic_adar_index(G, [(u,v)]))[2] or 0.0\n",
    "    except Exception:\n",
    "        aa = 0.0\n",
    "    \n",
    "    # Node Degrees: Number of edges incident to each node\n",
    "    # Provides information about node importance/centrality in the network\n",
    "    du = G.degree(u)\n",
    "    dv = G.degree(v)\n",
    "    \n",
    "    # Shortest Path Length: Minimum number of edges to traverse from u to v\n",
    "    # Capped at max_shortest to avoid expensive computations for disconnected nodes\n",
    "    try:\n",
    "        spl = nx.shortest_path_length(G, source=u, target=v)\n",
    "        if spl > max_shortest:\n",
    "            spl = max_shortest\n",
    "    except Exception:\n",
    "        # Nodes are disconnected (no path exists)\n",
    "        spl = max_shortest\n",
    "    \n",
    "    return [cn, j, aa, du, dv, spl]\n",
    "\n",
    "def build_pair_features(pairs: List[Tuple[int,int]], emb: np.ndarray, G: nx.Graph):\n",
    "    \"\"\"\n",
    "    Constructs a comprehensive feature matrix for node pairs by combining learned embeddings\n",
    "    with structural graph features.\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    for (u, v) in pairs:\n",
    "        # Extract learned embeddings for both nodes\n",
    "        zu = emb[u]\n",
    "        zv = emb[v]\n",
    "        \n",
    "        # Vector-based features from embeddings:\n",
    "        # - Concatenation of both embeddings (preserves individual node information)\n",
    "        # - Element-wise product (captures interaction patterns)\n",
    "        # - Absolute difference (captures similarity/dissimilarity)\n",
    "        vec = np.concatenate([zu, zv, zu * zv, np.abs(zu - zv)], axis=0)\n",
    "        \n",
    "        # Structural features from graph topology\n",
    "        sf = structural_features(G, u, v)\n",
    "        \n",
    "        # Combine embedding-based and structural features into single feature vector\n",
    "        feat = np.concatenate([vec, np.array(sf, dtype=float)], axis=0)\n",
    "        feats.append(feat)\n",
    "    \n",
    "    return np.vstack(feats)\n",
    "\n",
    "def sample_negatives(G: nx.Graph, num_samples: int, forbid_set:set):\n",
    "    \"\"\"\n",
    "    Samples random negative (non-edge) pairs from the graph for training.\n",
    "    \"\"\"\n",
    "    negs = set()\n",
    "    n = G.number_of_nodes()\n",
    "    \n",
    "    while len(negs) < num_samples:\n",
    "        u = random.randrange(n)\n",
    "        v = random.randrange(n)\n",
    "        \n",
    "        # Skip self-loops\n",
    "        if u == v:\n",
    "            continue\n",
    "        \n",
    "        # Normalize pair ordering (u < v) for consistent representation\n",
    "        key = (min(u, v), max(u, v))\n",
    "        \n",
    "        # Skip if this pair is forbidden (exists in training or future edges)\n",
    "        if key in forbid_set:\n",
    "            continue\n",
    "        \n",
    "        negs.add(key)\n",
    "    \n",
    "    return list(negs)\n",
    "\n",
    "def precision_at_k(y_true: np.ndarray, y_score: np.ndarray, K=100):\n",
    "    \"\"\"\n",
    "    Computes Precision@K metric for link prediction evaluation.\n",
    "    \"\"\"\n",
    "    idx = np.argsort(-y_score)[:K]\n",
    "    return y_true[idx].sum() / float(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSAGE encoder and pair-MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GraphSAGEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE encoder for learning node embeddings.\n",
    "    \n",
    "    The encoder uses GraphSAGE convolutional layers which:\n",
    "    - Sample a fixed-size neighborhood for each node\n",
    "    - Aggregate neighbor features\n",
    "    - Combine aggregated neighbor features with the node's own features\n",
    "    - Apply non-linear activation (ReLU) to learn complex patterns\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class PairMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (MLP) classifier for link prediction on node pairs.\n",
    "    \n",
    "    Architecture:\n",
    "        Input -> FC1 (hidden1) -> ReLU -> FC2 (hidden2) -> ReLU -> Output (1) -> Sigmoid\n",
    "    \n",
    "    Attributes:\n",
    "        fc1 (nn.Linear): First fully connected layer mapping input to first hidden dimension\n",
    "        fc2 (nn.Linear): Second fully connected layer mapping first to second hidden dimension\n",
    "        out (nn.Linear): Output layer mapping second hidden dimension to single logit\n",
    "    \n",
    "    Args:\n",
    "        in_dim (int): Dimensionality of input feature vectors (4*embedding_dim + 6)\n",
    "        hidden1 (int, optional): Size of first hidden layer. Defaults to 256.\n",
    "        hidden2 (int, optional): Size of second hidden layer. Defaults to 64.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden1=256, hidden2=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.out(x)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_delta = 1                   # [1,3,5]\n",
    "curr_vertex_degree_cutoff = 25      # [25,5,0]\n",
    "current_min_edges = 1               # [1,3]\n",
    "\n",
    "data_source=\"SemanticGraph_delta_\"+str(current_delta)+\"_cutoff_\"+str(curr_vertex_degree_cutoff)+\"_minedge_\"+str(current_min_edges)+\".pkl\"\n",
    "\n",
    "with open(data_source, \"rb\" ) as pkl_file:\n",
    "    full_dynamic_graph_sparse, unconnected_vertex_pairs, unconnected_vertex_pairs_solution, year_start, years_delta, vertex_degree_cutoff, min_edges = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create graph and node features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from(zip(full_dynamic_graph_sparse[:, 0],\n",
    "                     full_dynamic_graph_sparse[:, 1]))\n",
    "\n",
    "all_edges = list(G.edges())\n",
    "\n",
    "# sample a subset of edges to predict (between 50 and 800)\n",
    "future_edges = random.sample(all_edges, k=min(800, max(50, len(all_edges)//30)))\n",
    "\n",
    "# sample training edges not in future edges\n",
    "train_edges = [e for e in all_edges if (e not in future_edges and (e[1],e[0]) not in future_edges)]\n",
    "\n",
    "# Build training graph with all nodes but only training edges\n",
    "G_train = nx.Graph()\n",
    "G_train.add_nodes_from(G.nodes())\n",
    "G_train.add_edges_from(train_edges)\n",
    "\n",
    "n_nodes = G_train.number_of_nodes()\n",
    "\n",
    "deg = np.array([G_train.degree(i) for i in range(n_nodes)], dtype=float).reshape(-1,1)\n",
    "\n",
    "# Normalize degrees\n",
    "deg = deg / (deg.max() + 1e-9)\n",
    "\n",
    "rand_feat = np.random.randn(n_nodes, 16).astype(np.float32)\n",
    "node_features = np.concatenate([deg.astype(np.float32), rand_feat], axis=1)  # shape (n_nodes, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GraphSAGE encoder to learn node embeddings from graph structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "pyg_data = from_networkx(G_train)\n",
    "pyg_data.x = torch.tensor(node_features, dtype=torch.float32)\n",
    "pyg_data = pyg_data.to(device)\n",
    "\n",
    "encoder = GraphSAGEEncoder(in_channels=pyg_data.x.size(1), hidden_channels=128, num_layers=2).to(device)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "encoder.train()\n",
    "\n",
    "edge_index = pyg_data.edge_index\n",
    "\n",
    "# learn embeddings using link prediction as self-supervised task\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute node embeddings\n",
    "    emb = encoder(pyg_data.x, pyg_data.edge_index)\n",
    "    \n",
    "    # extract source and destination nodes for all edges\n",
    "    src = edge_index[0]\n",
    "    dst = edge_index[1]\n",
    "    \n",
    "    # similarity scores for positive pairs using dot product\n",
    "    pos_scores = (emb[src] * emb[dst]).sum(dim=1)\n",
    "    \n",
    "    # Positive loss\n",
    "    pos_loss = -F.logsigmoid(pos_scores).mean()\n",
    "    \n",
    "    # randomly sampled non-edges\n",
    "    n_pos = src.size(0)\n",
    "    \n",
    "    # Sample random negative pairs: keep source nodes, randomize destinations\n",
    "    neg_u = src[torch.randint(0, n_pos, (n_pos,)).to(device)]\n",
    "    neg_v = torch.randint(0, emb.size(0), (n_pos,)).to(device)\n",
    "    \n",
    "    # Compute similarity scores for negative pairs\n",
    "    neg_scores = (emb[neg_u] * emb[neg_v]).sum(dim=1)\n",
    "    \n",
    "    # Negative loss\n",
    "    neg_loss = -F.logsigmoid(-neg_scores).mean()\n",
    "    \n",
    "    # Total loss\n",
    "    loss = pos_loss + neg_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch} loss {loss.item():.4f}\")\n",
    "\n",
    "# final node embeddings\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    node_emb = encoder(pyg_data.x, pyg_data.edge_index).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normalize positive pairs to ensure consistent ordering (u < v), remove duplicates\n",
    "positives = [ (min(u,v), max(u,v)) for (u,v) in future_edges ]\n",
    "positives = list(dict.fromkeys(positives))\n",
    "\n",
    "# forbidden set\n",
    "train_set = { (min(u,v), max(u,v)) for (u,v) in G_train.edges() }\n",
    "forbid = set(train_set) | set(positives)\n",
    "\n",
    "# Sample negative examples\n",
    "negatives = sample_negatives(G_train, num_samples=len(positives), forbid_set=forbid)\n",
    "print(\"Pairs: positives:\", len(positives), \"negatives:\", len(negatives))\n",
    "\n",
    "pairs_all = positives + negatives\n",
    "\n",
    "# feature matrix: embeddings, embedding interactions, and structural features\n",
    "X = build_pair_features(pairs_all, node_emb, G_train)\n",
    "\n",
    "# binary labels: 1 for positive pairs (future edges), 0 for negative pairs (non-edges)\n",
    "y = np.hstack([np.ones(len(positives)), np.zeros(len(negatives))])\n",
    "print(\"Feature matrix shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train MLP classifier to predict link existence from pair features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "model = PairMLP(X_train.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "batch_size = 128\n",
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle training data each epoch for better generalization\n",
    "    perm = torch.randperm(X_train_t.size(0))\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(0, X_train_t.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        xb = X_train_t[idx]\n",
    "        yb = y_train_t[idx]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "     \n",
    "        loss = F.binary_cross_entropy(out, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    # evaluation on validation set \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_val_t).cpu().numpy()\n",
    "        auc = roc_auc_score(y_val, preds)\n",
    "        ap = average_precision_score(y_val, preds)\n",
    "    model.train()\n",
    "    print(f\"Epoch {epoch} train_loss={np.mean(losses):.4f} val_auc={auc:.4f} val_ap={ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation on the complete dataset (training + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_all = model(torch.tensor(X_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "auc_all = roc_auc_score(y, preds_all)\n",
    "ap_all = average_precision_score(y, preds_all)\n",
    "prec100 = precision_at_k(y, preds_all, K=min(100, len(y)))\n",
    "print(\"Final metrics: AUC\", auc_all, \"AP\", ap_all, \"Precision@100\", prec100)\n",
    "\n",
    "# Save predictions to CSV file\n",
    "out_df = pd.DataFrame({\n",
    "    \"u\": [p[0] for p in pairs_all],\n",
    "    \"v\": [p[1] for p in pairs_all],\n",
    "    \"label\": list(y.astype(int)), # 1=edge, 0=non-edge\n",
    "    \"score\": list(preds_all)\n",
    "})\n",
    "os.makedirs(os.path.dirname(save_predictions_path), exist_ok=True)\n",
    "out_df.to_csv(save_predictions_path, index=False)\n",
    "print(\"Saved predictions to\", save_predictions_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
